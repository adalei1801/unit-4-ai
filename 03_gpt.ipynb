{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 2.00Git/s]                                                     \n",
      "Fetching encoder.json: 1.05Mit [00:00, 2.89Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 4.76Git/s]                                                   \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [02:33, 3.25Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 1.95Git/s]                                               \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.04Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 4.40Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "# gpt2.download_gpt2(\n",
    "#    model_name = '124M'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "session = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(session, model_name='124M', reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the book ran off to bermuda after the 19th century and was published by the New York Times in 1859. It was later rereading by the publisher Franklin D. Roosevelt.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "The issue is a highlight of this year's event, which features a fondness for all things German—and so it's not a real surprise to find some German athletes in attendance.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "You can read more about the event here.\n",
      "\n",
      "The Independent has teamed up with WIRED for a feature on the events and sponsors.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "Advertisement<|endoftext|>The first U.S.-made Olympus OM-D EOS 5D Mark II was released in 1997 and is one of the best-looking cameras of all time. The 5D Mark II, which is designed to rival both the Leica M1 and the Leica E, produces more than 14,000 square-foot images per second, and now has twice the resolution compared to the previous model. The first model's 5D Mark II was so powerful that it was certified as a class 1 camera, and its fifth-generation i-MAX sensor is now used by more than 100,000 people worldwide.\n",
      "\n",
      "The 5D Mark II is the best looking camera in the world and is more than capable of capturing more than 20,000 images per second.\n",
      "\n",
      "The new 5D Mark II is a 1440p/60p/40p version of the M1 for the Sony FE lens. The lens is completely redesigned. The new design was developed by Sony and was developed in partnership with the Fuji TMC and the DMC. The main differences between the M1 and the M1 were the inclusion of an optical f/5.6 lens and a 90mm f/1.4 wide angle lens. The M1 is now the world's biggest camera and the 5D Mark II is the world's first high-end camera with a lens that is more than capable of capturing live-action images.\n",
      "\n",
      "The new 5D Mark II is a 13-megapixel camera with a f/4 aperture and a 3:2:2 aspect ratio, an extremely sharp and natural image sensor, and an in-built 90mm f/2.8\n",
      "\n",
      "The lens also has a Canon EOS 35mm f/1.4, f/2.8, and f/2.8 mount, and the 5D Mark II is equipped with a special lens with a built-in 20mm focal length zoom.\n",
      "\n",
      "The new EOS 5D Mark II is an extremely bright, wide-angle camera. It features a full HD display, which is the second fastest growing camera on the market. The exact same 3D camera is used in the new 5D Mark II.\n",
      "\n",
      "The new 5D Mark II is a 14-megapixel camera with a f/2.8 lens and a 2:2:2 aperture.\n",
      "\n",
      "The lens is held in place by a double-sided tapered lens. The lens is placed on the chassis, and the lens is held in place with a rubber pad. The stitching is smooth, and the stitching is very fast. The image quality is very impressive.\n",
      "\n",
      "The new 5D Mark II is equipped with a 12-megapixel camera. The new camera is 16-megapixels and the camera is designed to capture images with an ISO of 300. The camera is rated at a resolution of 5,000:1. The camera is also with a digital image stabilization system. The ISO is set at ISO 400.\n",
      "\n",
      "The new 5D Mark II is equipped with a fast-charging 3.5-inch LCD screen with improved image processing capabilities.\n",
      "\n",
      "The new 5D Mark II is equipped with a 2.8-megapixel camera, a fast-charging 3.5-inch LCD screen with improved image processing capabilities, and a built-in 2.8-megapixel camera that offers more resolution and power. The camera is rated at a resolution of 1,250:1.\n",
      "\n",
      "The camera is rated at a resolution of 1,100:1.\n",
      "\n",
      "The new 5D Mark II will be available from Polar and will be available in the US from December 17th, 2018.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "SOURCE: Polar<|endoftext|>INSIDE A world of lush gardens, a gorgeously designed castle, and one of the most important cities in the world, in St. Petersburg, Russia, is a master of the art of the city's architecture.\n",
      "\n",
      "In 1904, St. Petersburg's Prince Vladimir was awarded the Order of the Great Star by the Russian Empire. Not only did he, but a certain English nobleman named James T. Evans (1808–1934), author of The Cathedral of St. Petersburg, have the honor of serving as the prince's footman during his time at the palace, but also a substantial number of the city's most famous residents, including Prince Petersburg and a number of other prominent citizens of St. Petersburg\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(\n",
    "    session, \n",
    "    model_name = '124M',\n",
    "    prefix='the book ran off to bermuda'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shakespeare GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session2 = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732547174.237549 4731234 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 338024 tokens\n",
      "Training...\n",
      "[1 | 20.65] loss=4.07 avg=4.07\n",
      "[2 | 40.80] loss=4.06 avg=4.06\n",
      "[3 | 59.94] loss=3.51 avg=3.88\n",
      "[4 | 78.96] loss=3.70 avg=3.83\n",
      "[5 | 97.40] loss=3.90 avg=3.85\n",
      "[6 | 118.06] loss=3.38 avg=3.77\n",
      "[7 | 136.79] loss=3.73 avg=3.76\n",
      "[8 | 155.65] loss=3.75 avg=3.76\n",
      "[9 | 174.61] loss=3.59 avg=3.74\n",
      "[10 | 193.13] loss=3.89 avg=3.76\n",
      "[11 | 210.83] loss=3.73 avg=3.75\n",
      "[12 | 228.61] loss=3.69 avg=3.75\n",
      "[13 | 245.56] loss=3.55 avg=3.73\n",
      "[14 | 262.67] loss=3.57 avg=3.72\n",
      "[15 | 280.53] loss=3.56 avg=3.71\n",
      "[16 | 299.03] loss=3.77 avg=3.71\n",
      "[17 | 317.89] loss=3.38 avg=3.69\n",
      "[18 | 336.47] loss=3.77 avg=3.70\n",
      "[19 | 357.54] loss=3.71 avg=3.70\n",
      "[20 | 376.97] loss=3.64 avg=3.69\n",
      "[21 | 395.32] loss=3.51 avg=3.68\n",
      "[22 | 413.76] loss=3.44 avg=3.67\n",
      "[23 | 432.14] loss=3.29 avg=3.65\n",
      "[24 | 450.29] loss=3.60 avg=3.65\n",
      "[25 | 470.03] loss=3.55 avg=3.65\n",
      "[26 | 490.52] loss=3.59 avg=3.64\n",
      "[27 | 509.35] loss=3.34 avg=3.63\n",
      "[28 | 527.67] loss=3.35 avg=3.62\n",
      "[29 | 547.09] loss=3.34 avg=3.61\n",
      "[30 | 567.45] loss=3.30 avg=3.60\n",
      "[31 | 587.24] loss=3.56 avg=3.59\n",
      "[32 | 607.76] loss=3.35 avg=3.59\n",
      "[33 | 627.45] loss=3.42 avg=3.58\n",
      "[34 | 647.93] loss=3.41 avg=3.57\n",
      "[35 | 669.41] loss=3.61 avg=3.57\n",
      "[36 | 689.32] loss=3.45 avg=3.57\n",
      "[37 | 709.56] loss=3.36 avg=3.56\n",
      "[38 | 728.86] loss=3.22 avg=3.55\n",
      "[39 | 748.26] loss=3.18 avg=3.54\n",
      "[40 | 768.57] loss=3.18 avg=3.53\n",
      "[41 | 789.04] loss=3.48 avg=3.53\n",
      "[42 | 809.27] loss=3.43 avg=3.53\n",
      "[43 | 830.19] loss=3.19 avg=3.52\n",
      "[44 | 850.68] loss=3.21 avg=3.51\n",
      "[45 | 870.96] loss=3.24 avg=3.50\n",
      "[46 | 890.70] loss=3.31 avg=3.50\n",
      "[47 | 910.70] loss=3.45 avg=3.49\n",
      "[48 | 930.30] loss=3.28 avg=3.49\n",
      "[49 | 948.89] loss=2.97 avg=3.48\n",
      "[50 | 967.41] loss=3.28 avg=3.47\n",
      "[51 | 987.11] loss=3.34 avg=3.47\n",
      "[52 | 1006.94] loss=3.24 avg=3.46\n",
      "[53 | 1026.18] loss=3.37 avg=3.46\n",
      "[54 | 1045.00] loss=3.36 avg=3.46\n",
      "[55 | 1064.57] loss=3.54 avg=3.46\n",
      "[56 | 1083.21] loss=3.32 avg=3.46\n",
      "[57 | 1102.27] loss=3.28 avg=3.45\n",
      "[58 | 1120.98] loss=3.46 avg=3.45\n",
      "[59 | 1139.86] loss=3.13 avg=3.44\n",
      "[60 | 1158.98] loss=3.51 avg=3.45\n",
      "[61 | 1178.63] loss=3.36 avg=3.44\n",
      "[62 | 1197.90] loss=3.21 avg=3.44\n",
      "[63 | 1217.13] loss=3.16 avg=3.43\n",
      "[64 | 1236.76] loss=3.48 avg=3.43\n",
      "[65 | 1257.92] loss=3.19 avg=3.43\n",
      "[66 | 1277.28] loss=3.29 avg=3.43\n",
      "[67 | 1298.06] loss=3.25 avg=3.42\n",
      "[68 | 1318.45] loss=3.09 avg=3.42\n",
      "[69 | 1338.55] loss=3.14 avg=3.41\n",
      "[70 | 1358.32] loss=3.34 avg=3.41\n",
      "[71 | 1377.35] loss=3.29 avg=3.41\n",
      "[72 | 1396.05] loss=3.30 avg=3.40\n",
      "[73 | 1414.51] loss=3.14 avg=3.40\n",
      "[74 | 1433.39] loss=2.93 avg=3.39\n",
      "[75 | 1451.92] loss=3.07 avg=3.38\n",
      "[76 | 1470.15] loss=3.20 avg=3.38\n",
      "[77 | 1490.48] loss=3.25 avg=3.38\n",
      "[78 | 1509.94] loss=3.19 avg=3.37\n",
      "[79 | 1529.08] loss=3.24 avg=3.37\n",
      "[80 | 1548.05] loss=3.39 avg=3.37\n",
      "[81 | 1567.21] loss=3.17 avg=3.37\n",
      "[82 | 1586.72] loss=2.91 avg=3.36\n",
      "[83 | 1606.35] loss=3.31 avg=3.36\n",
      "[84 | 1624.77] loss=3.45 avg=3.36\n",
      "[85 | 1643.34] loss=3.41 avg=3.36\n",
      "[86 | 1661.66] loss=3.37 avg=3.36\n",
      "[87 | 1680.32] loss=3.30 avg=3.36\n",
      "[88 | 1701.21] loss=3.38 avg=3.36\n",
      "[89 | 1720.77] loss=3.11 avg=3.36\n",
      "[90 | 1739.14] loss=3.23 avg=3.36\n",
      "[91 | 1758.93] loss=3.15 avg=3.35\n",
      "[92 | 1778.28] loss=3.24 avg=3.35\n",
      "[93 | 1797.56] loss=3.11 avg=3.35\n",
      "[94 | 1816.30] loss=3.18 avg=3.34\n",
      "[95 | 1835.11] loss=3.19 avg=3.34\n",
      "[96 | 1854.12] loss=3.22 avg=3.34\n",
      "[97 | 1872.62] loss=3.02 avg=3.33\n",
      "[98 | 1891.16] loss=2.97 avg=3.33\n",
      "[99 | 1909.61] loss=3.15 avg=3.33\n",
      "[100 | 1927.69] loss=2.93 avg=3.32\n",
      "Saving checkpoint/shakespeare/model-100\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(\n",
    "    session2,\n",
    "    'shakespeare.txt',\n",
    "    model_name = '124M',\n",
    "    steps=100,\n",
    "    run_name='shakespeare'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashley: Milan was stolen with her father;\n",
      "Which, like her father, she hath taken with her mother's\n",
      "sire; but she hath not done with her own;\n",
      "And so it is; hence, for our sake,\n",
      "I'll make the issue of this business thorough;\n",
      "In this place I will make my way,\n",
      "For our party to the town; where we shall\n",
      "Be seated, and so proceed and attend to\n",
      "the business.\n",
      "\n",
      "BAPTISTA:\n",
      "Well, I'll bid you resolve\n",
      "That you should, with the help of the people\n",
      "The most virtuous thing you could do in\n",
      "your absence, and so obey the people with\n",
      "your best interest, as you would do in the\n",
      "place where you live; and here I will\n",
      "Provide a method of instructing you.\n",
      "\n",
      "MENENIUS:\n",
      "You shall have a minister in your stead.\n",
      "\n",
      "BAPTISTA:\n",
      "Your duty is to be a friend;\n",
      "And if you should have your reason,\n",
      "If you should have your cause, you shall\n",
      "Be a friend to the people, not to yourselves.\n",
      "\n",
      "MENENIUS:\n",
      "I do so, sir;\n",
      "And to your knowledge, as you do to your\n",
      "father and mother, I will you, with\n",
      "your best interest, answer to my request.\n",
      "\n",
      "BAPTISTA:\n",
      "This is my way.\n",
      "\n",
      "MENENIUS:\n",
      "You shall have a minister in your stead.\n",
      "\n",
      "BAPTISTA:\n",
      "Your duty is to be a friend;\n",
      "And if you should have your reason,\n",
      "If you should have your cause, you shall\n",
      "Be a friend to the people, not to yourself.\n",
      "\n",
      "MENENIUS:\n",
      "You shall have a minister in your stead.\n",
      "\n",
      "BAPTISTA:\n",
      "You shall have a minister in your stead.\n",
      "\n",
      "MENENIUS:\n",
      "Your duty is to be a friend;\n",
      "And if you should have your cause,\n",
      "If you should have your cause, you shall\n",
      "Be a friend to the people, not to yourself.\n",
      "\n",
      "BAPTISTA:\n",
      "This is my way;\n",
      "And to your knowledge, as you do to your\n",
      "father and mother, I will you, with your best\n",
      "interest, answer to my request.\n",
      "\n",
      "MENENIUS:\n",
      "For my part, sir, I will answer to your\n",
      "quotations.\n",
      "\n",
      "BAPTISTA:\n",
      "This is my way.\n",
      "\n",
      "MENENIUS:\n",
      "You shall have a minister in your stead.\n",
      "\n",
      "BAPTISTA:\n",
      "Your duty is to be a friend;\n",
      "And if you should have your cause,\n",
      "You shall be a friend to the people, not to yourselves.\n",
      "\n",
      "MENENIUS:\n",
      "Go, go.\n",
      "\n",
      "BAPTISTA:\n",
      "You shall have a minister in your stead.\n",
      "\n",
      "MENENIUS:\n",
      "Sometime in this year, sir,\n",
      "I shall make use of this gentleman.\n",
      "I mean to be your friend.\n",
      "\n",
      "BAPTISTA:\n",
      "You shall have a minister in your stead.\n",
      "\n",
      "MENENIUS:\n",
      "You shall have a minister in your stead.\n",
      "\n",
      "BAPTISTA:\n",
      "You shall have ministers in your stead.\n",
      "\n",
      "MENENIUS:\n",
      "Provost,\n",
      "Your brother, I am sorry for your loss!\n",
      "I am sorry for your loss,\n",
      "In whose stead I have been absent so long.\n",
      "\n",
      "BAPTISTA:\n",
      "I am sorry for your loss,\n",
      "In whose stead I have been absent so long.\n",
      "\n",
      "MENENIUS:\n",
      "Your brother, I am sorry for your loss!\n",
      "I am sorry for your loss,\n",
      "In whose stead I have been absent so long.\n",
      "\n",
      "BAPTISTA:\n",
      "Go, go.\n",
      "\n",
      "MENENIUS:\n",
      "This is the way of this gentleman:\n",
      "And to your knowledge, as you do to your\n",
      "father and mother, I will you, with your best interest\n",
      "have the time of your absence.\n",
      "\n",
      "BAPTISTA:\n",
      "You shall have other ministers in your stead.\n",
      "\n",
      "MENENIUS:\n",
      "Provost,\n",
      "My brother, I am sorry for your loss!\n",
      "I am sorry for your loss,\n",
      "In whose stead I have been absent so long.\n",
      "\n",
      "BAPTISTA:\n",
      "I am sorry for your loss, in whose stead\n",
      "I have been absent so long.\n",
      "\n",
      "MENENIUS:\n",
      "Your brother, I am sorry for your loss!\n",
      "My brother, I am sorry for your loss!\n",
      "My brother, I am sorry for your loss!\n",
      "\n",
      "BAPTISTA:\n",
      "Provost,\n",
      "My brother, I am sorry for your loss!\n",
      "My brother, I am sorry for your loss\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(\n",
    "    session2,\n",
    "    prefix='Ashley: Milan was stolen',\n",
    "    run_name='shakespeare'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
